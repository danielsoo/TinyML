{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† TinyML - Colab Training Notebook\n",
    "\n",
    "**Version:** V.1.3.13 - Optimized for main branch\n",
    "\n",
    "This notebook trains your TinyML model on Google Colab GPU.\n",
    "\n",
    "**Flow:**\n",
    "1. ‚úÖ Check GPU availability\n",
    "2. üì• Clone or update from GitHub (main branch)\n",
    "3. üì¶ Install Colab-compatible dependencies\n",
    "4. üöÄ Run training script (`scripts/train.py`) ‚Üí saves `src/models/global_model.h5`\n",
    "5. üíæ Save exported models to Google Drive\n",
    "\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Uses `main` branch (latest stable code)- TFLite export support\n",
    "\n",
    "- Automatic dependency management- Timestamped model preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å Ïó∞Í≤∞\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ Ìè¥Îçî Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "DATA_DIR = \"/content/drive/MyDrive/TinyML_models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Runtime & GPU check\n",
    "Make sure you set **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU** before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi || echo \"No NVIDIA GPU detected. Please enable GPU in Runtime settings.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone or update TinyML repository\n",
    "\n",
    "Set your GitHub repo URL if different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/danielsoo/TinyML.git\"  # change if needed\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "\n",
    "print(\"üîÑ Updating repository from GitHub...\")\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print(f\"üì• Cloning repository from {REPO_URL}...\")\n",
    "    result = subprocess.run([\"git\", \"clone\", REPO_URL, PROJECT_DIR], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    print(\"‚úÖ Repository cloned successfully\")\n",
    "else:\n",
    "    print(f\"üì• Pulling latest changes from {REPO_URL}...\")\n",
    "    # Change to project directory and pull\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    # Fetch latest changes\n",
    "    subprocess.run([\"git\", \"fetch\", \"origin\"], \n",
    "                  capture_output=True, text=True, check=False)\n",
    "    # Pull latest changes\n",
    "    result = subprocess.run([\"git\", \"pull\", \"origin\", \"main\"], \n",
    "                          capture_output=True, text=True, check=False)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Repository updated successfully\")\n",
    "        if result.stdout.strip():\n",
    "            print(f\"   Changes: {result.stdout.strip()[:100]}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Git pull had issues (may be up to date): {result.stderr[:100]}\")\n",
    "        # Try to continue anyway\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Show current commit\n",
    "commit_result = subprocess.run([\"git\", \"log\", \"-1\", \"--oneline\"], \n",
    "                              capture_output=True, text=True, check=False)\n",
    "if commit_result.returncode == 0:\n",
    "    print(f\"üìå Current commit: {commit_result.stdout.strip()}\")\n",
    "\n",
    "# Add project directory to Python path for module imports\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "print(f\"\\n‚úÖ Project directory ready: {PROJECT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4756a1",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£.5 Update config data path\n",
    "Google DriveÏóê ÏûàÎäî Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú(`DATA_DIR`)Î•º `config/federated_colab.yaml`Ïóê Î∞òÏòÅÌï©ÎãàÎã§.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231529f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "config_path = Path(PROJECT_DIR) / \"config\" / \"federated_colab.yaml\"\n",
    "\n",
    "if config_path.exists():\n",
    "    with config_path.open(\"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    cfg.setdefault(\"data\", {})\n",
    "    cfg[\"data\"][\"path\"] = DATA_DIR\n",
    "\n",
    "    with config_path.open(\"w\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "    print(f\"Updated federated_colab.yaml data.path -> {cfg['data']['path']}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cannot find {config_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Install Colab-compatible dependencies\n",
    "\n",
    "**IMPORTANT:** After running the next cell, you MUST restart the runtime!\n",
    "- Go to: **Runtime ‚Üí Restart runtime**\n",
    "- Then skip directly to the training cell (Cell 6Ô∏è‚É£)\n",
    "\n",
    "This ensures protobuf version stays fixed at 3.20.3 for TensorFlow compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Install dependencies (Colab compatible)\n",
    "\n",
    "- Installs from `colab_requirements.txt` if present.\n",
    "- Installs standard `tensorflow` for Linux GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Install from colab-specific requirements\n",
    "colab_req_file = os.path.join(PROJECT_DIR, \"colab/requirements_colab.txt\")\n",
    "if os.path.exists(colab_req_file):\n",
    "    print(f\"   Installing from {colab_req_file}...\")\n",
    "    subprocess.run([\"pip\", \"install\", \"-r\", colab_req_file], check=False)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  {colab_req_file} not found.\")\n",
    "\n",
    "# Fix protobuf version FIRST before other packages\n",
    "print(\"\\nüîß Fixing protobuf compatibility...\")\n",
    "subprocess.run([\"pip\", \"uninstall\", \"-y\", \"protobuf\"], check=False)\n",
    "subprocess.run([\"pip\", \"install\", \"protobuf==3.20.3\"], check=False)\n",
    "\n",
    "# Install compatible flwr version (without deps to avoid protobuf upgrade)\n",
    "print(\"   Installing Flower with compatible version...\")\n",
    "subprocess.run([\"pip\", \"install\", \"flwr==1.6.0\", \"--no-deps\"], check=False)\n",
    "\n",
    "# Install flwr dependencies separately\n",
    "subprocess.run([\n",
    "    \"pip\", \"install\",\n",
    "    \"cryptography<42.0.0,>=41.0.2\",\n",
    "    \"grpcio!=1.52.0,<2.0.0,>=1.48.2\",\n",
    "    \"iterators>=0.0.2,<0.0.3\",\n",
    "    \"numpy>=1.21.0,<2.0.0\",\n",
    "    \"pycryptodome>=3.18.0,<4.0.0\"\n",
    "], check=False)\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed successfully\")\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: Restart runtime to apply changes!\")\n",
    "print(\"   Go to: Runtime ‚Üí Restart runtime\")\n",
    "print(\"   Then skip to the training cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation and check compatibility\n",
    "import tensorflow as tf\n",
    "import google.protobuf\n",
    "\n",
    "print(\"‚úÖ TensorFlow version:\", tf.__version__)\n",
    "print(\"‚úÖ Protobuf version:\", google.protobuf.__version__)\n",
    "print(\"‚úÖ GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Check if protobuf version is correct\n",
    "if google.protobuf.__version__ != \"3.20.3\":\n",
    "    print(\"‚ö†Ô∏è  WARNING: Protobuf version mismatch!\")\n",
    "    print(\"   Please restart runtime and skip to training cell.\")\n",
    "else:\n",
    "    print(\"‚úÖ All versions compatible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ (Optional) Download or prepare dataset\n",
    "\n",
    "Edit this cell if your training script expects data in a specific path.\n",
    "For example, you can mount Google Drive or download from Kaggle here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: mount Google Drive if your data is stored there.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Example: create a data directory\n",
    "# os.makedirs('data', exist_ok=True)\n",
    "# Then copy or download your dataset into ./data\n",
    "\n",
    "print(\"Dataset preparation step: customize as needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Run training\n",
    "\n",
    "This cell tries to run `train.py` at repo root.\n",
    "If your main script is at a different path, edit accordingly (e.g. `src/train.py`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe73708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/content/drive/MyDrive/TinyML_models\")  # CSVÎì§Ïù¥ ÏûàÎäî Í≤ΩÎ°úÎ°ú ÏàòÏ†ï\n",
    "csv_paths = sorted(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "dfs = [pd.read_csv(p, low_memory=False) for p in csv_paths]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Total Samples:\", len(df))\n",
    "print(df[\"attack\"].value_counts())\n",
    "print(df[\"attack\"].value_counts(normalize=True))  # ÎπÑÏú® ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Add project directory to Python path\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "# Use train.py script (unified training script)\n",
    "# This will automatically detect Colab environment and use federated_colab.yaml\n",
    "print(\"üöÄ Running training with scripts/train.py...\")\n",
    "print(\"   This will use config/federated_colab.yaml automatically\")\n",
    "print(\"   You will see training progress below...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use ! command for real-time output in Colab\n",
    "!python scripts/train.py --config config/federated_colab.yaml\n",
    "\n",
    "# Check if model was actually saved\n",
    "from pathlib import Path\n",
    "model_path = Path(PROJECT_DIR) / \"src\" / \"models\" / \"global_model.h5\"\n",
    "model_exists = model_path.exists() and model_path.stat().st_size > 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if model_exists:\n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    print(f\"   Model saved to: src/models/global_model.h5\")\n",
    "    # Show model size\n",
    "    size_mb = model_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"   Model size: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå Training failed!\")\n",
    "    print(\"   Model file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694b385",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Model Compression\n",
    "\n",
    "Apply pruning and quantization to compress the trained model.\n",
    "This will create multiple compressed versions for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply compression without evaluation (to avoid data shape mismatch)\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "# Check if model exists\n",
    "model_path = \"src/models/global_model.h5\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"‚ö†Ô∏è  WARNING: No trained model found!\")\n",
    "    print(\"‚ö†Ô∏è  Please run the training step first.\")\n",
    "else:\n",
    "    from src.modelcompression.pruning import apply_structured_pruning\n",
    "    \n",
    "    # Load model\n",
    "    print(\"üì¶ Loading trained model...\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"‚úÖ Model loaded (input shape: {model.input_shape})\\n\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(\"models/tflite\", exist_ok=True)\n",
    "    \n",
    "    # 1. Export baseline TFLite (Float32)\n",
    "    print(\"üíæ Step 1: Exporting Baseline TFLite (Float32)\")\n",
    "    print(\"-\" * 60)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    baseline_tflite = converter.convert()\n",
    "    baseline_path = \"models/tflite/saved_model_original.tflite\"\n",
    "    with open(baseline_path, \"wb\") as f:\n",
    "        f.write(baseline_tflite)\n",
    "    baseline_size = len(baseline_tflite) / 1024\n",
    "    print(f\"‚úÖ Saved: {baseline_path} ({baseline_size:.2f} KB)\\n\")\n",
    "    \n",
    "    # 2. Apply Pruning\n",
    "    print(\"‚úÇÔ∏è  Step 2: Applying Structured Pruning (50% pruning_ratio)\")\n",
    "    print(\"-\" * 60)\n",
    "    pruned_model = apply_structured_pruning(model, pruning_ratio=0.5)\n",
    "    \n",
    "    # Save pruned H5\n",
    "    pruned_h5_path = \"models/test_pruned_model.h5\"\n",
    "    pruned_model.save(pruned_h5_path)\n",
    "    print(f\"‚úÖ Saved pruned model: {pruned_h5_path}\\n\")\n",
    "    \n",
    "    # 3. Export pruned TFLite (Float32)\n",
    "    print(\"üíæ Step 3: Exporting Pruned TFLite (Float32)\")\n",
    "    print(\"-\" * 60)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "    pruned_tflite = converter.convert()\n",
    "    pruned_path = \"models/tflite/saved_model_pruned.tflite\"\n",
    "    with open(pruned_path, \"wb\") as f:\n",
    "        f.write(pruned_tflite)\n",
    "    pruned_size = len(pruned_tflite) / 1024\n",
    "    print(f\"‚úÖ Saved: {pruned_path} ({pruned_size:.2f} KB)\\n\")\n",
    "    \n",
    "    # 4. Apply INT8 Quantization to TFLite\n",
    "    print(\"‚ö° Step 4: Applying INT8 Quantization\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Create dummy representative dataset (since we can't load actual data)\n",
    "    input_shape = model.input_shape[1:]\n",
    "    def representative_dataset():\n",
    "        for _ in range(100):\n",
    "            data = np.random.randn(1, *input_shape).astype(np.float32)\n",
    "            yield [data]\n",
    "    \n",
    "    # Apply INT8 quantization using TFLite converter\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    \n",
    "    quantized_tflite = converter.convert()\n",
    "    \n",
    "    quantized_path = \"models/tflite/saved_model_pruned_quantized.tflite\"\n",
    "    with open(quantized_path, \"wb\") as f:\n",
    "        f.write(quantized_tflite)\n",
    "    quantized_size = len(quantized_tflite) / 1024\n",
    "    \n",
    "    print(f\"‚úÖ Saved: {quantized_path} ({quantized_size:.2f} KB)\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚úÖ COMPRESSION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüìä Size Comparison:\")\n",
    "    print(f\"  Baseline TFLite:    {baseline_size:.2f} KB\")\n",
    "    print(f\"  Pruned TFLite:      {pruned_size:.2f} KB ({baseline_size/pruned_size:.2f}x)\")\n",
    "    print(f\"  Quantized TFLite:   {quantized_size:.2f} KB ({baseline_size/quantized_size:.2f}x)\")\n",
    "    print(f\"\\nüíæ Compressed models saved to:\")\n",
    "    print(f\"  - {pruned_h5_path}\")\n",
    "    print(f\"  - {baseline_path}\")\n",
    "    print(f\"  - {pruned_path}\")\n",
    "    print(f\"  - {quantized_path}\")\n",
    "    print(f\"\\nüìù Note: Accuracy evaluation will be done in analysis step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£.5 Compression Analysis\n",
    "\n",
    "Analyze the compressed models: size, accuracy, and inference speed.\n",
    "Generate comparison reports and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfa898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run compression analysis on all models\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Add project directory to Python path\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "# Analyze all compressed models\n",
    "models_to_analyze = [\n",
    "    \"Baseline:src/models/global_model.h5\",\n",
    "    \"Baseline-TFLite:models/global_model.tflite\",\n",
    "    \"Pruned:models/pruned_model.h5\",\n",
    "    \"Pruned-TFLite:models/pruned_model.tflite\",\n",
    "    \"Pruned+Quantized:models/pruned_quantized.tflite\"\n",
    "]\n",
    "\n",
    "# Filter only existing models\n",
    "existing_models = []\n",
    "for model_spec in models_to_analyze:\n",
    "    model_path = model_spec.split(\":\", 1)[1]\n",
    "    if os.path.exists(model_path):\n",
    "        existing_models.append(model_spec)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Model not found: {model_path}\")\n",
    "\n",
    "if not existing_models:\n",
    "    print(\"‚ùå No models found for analysis!\")\n",
    "    print(\"Please run the compression step first.\")\n",
    "else:\n",
    "    models_str = \" \".join([f'\"{m}\"' for m in existing_models])\n",
    "    \n",
    "    cmd = f\"\"\"python scripts/analyze_compression.py \\\n",
    "        --models {models_str} \\\n",
    "        --baseline src/models/global_model.h5 \\\n",
    "        --config config/federated_colab.yaml \\\n",
    "        --output-dir data/processed/analysis \\\n",
    "        --format all\"\"\"\n",
    "    \n",
    "    print(\"Running compression analysis...\")\n",
    "    print(f\"Analyzing {len(existing_models)} models:\\n\")\n",
    "    for model in existing_models:\n",
    "        print(f\"  ‚úì {model}\")\n",
    "    print()\n",
    "    \n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a515a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FGSM attack testing on compressed models\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Add project directory to Python path\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "# Check if models exist\n",
    "baseline_model = \"src/models/global_model.h5\"\n",
    "pruned_model = \"models/pruned_model.h5\"\n",
    "\n",
    "if not os.path.exists(baseline_model):\n",
    "    print(\"‚ö†Ô∏è  WARNING: No trained model found!\")\n",
    "    print(\"‚ö†Ô∏è  Please run the training step first.\")\n",
    "else:\n",
    "    print(\"üîí Testing adversarial robustness with FGSM attack...\")\n",
    "    print(\"This evaluates model security before and after compression.\\n\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"Testing Baseline Model:\")\n",
    "    print(\"-\" * 60)\n",
    "    !python scripts/test_fgsm_attack.py --model {baseline_model}\n",
    "    \n",
    "    if os.path.exists(pruned_model):\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"Testing Pruned Model:\")\n",
    "        print(\"-\" * 60)\n",
    "        !python scripts/test_fgsm_attack.py --model {pruned_model}\n",
    "    \n",
    "    print(\"\\n‚úÖ FGSM testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy compressed models and analysis to Google Drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Copy compressed models\n",
    "models_to_copy = [\n",
    "    \"models/pruned_model.h5\",\n",
    "    \"models/pruned_model.tflite\",\n",
    "    \"models/pruned_quantized.tflite\",\n",
    "    \"models/global_model.tflite\"\n",
    "]\n",
    "\n",
    "drive_models_dir = \"/content/drive/MyDrive/TinyML_models\"\n",
    "os.makedirs(drive_models_dir, exist_ok=True)\n",
    "\n",
    "print(\"üì¶ Copying compressed models to Google Drive...\")\n",
    "for model_path in models_to_copy:\n",
    "    if os.path.exists(model_path):\n",
    "        dst = os.path.join(drive_models_dir, os.path.basename(model_path))\n",
    "        shutil.copy(model_path, dst)\n",
    "        print(f\"‚úÖ Copied: {os.path.basename(model_path)}\")\n",
    "\n",
    "# Copy analysis results\n",
    "analysis_dir = \"data/processed/analysis\"\n",
    "drive_analysis_dir = \"/content/drive/MyDrive/TinyML_models/analysis\"\n",
    "os.makedirs(drive_analysis_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(analysis_dir):\n",
    "    print(\"\\nüìä Copying analysis results...\")\n",
    "    for file in os.listdir(analysis_dir):\n",
    "        src = os.path.join(analysis_dir, file)\n",
    "        dst = os.path.join(drive_analysis_dir, file)\n",
    "        if os.path.isfile(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"‚úÖ Copied: {file}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All files saved to Google Drive!\")\n",
    "print(f\"   Models: {drive_models_dir}\")\n",
    "print(f\"   Analysis: {drive_analysis_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display visualizations inline\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "analysis_dir = \"data/processed/analysis\"\n",
    "plots = [\n",
    "    \"size_vs_accuracy.png\",\n",
    "    \"compression_metrics.png\",\n",
    "    \"compression_ratio.png\"\n",
    "]\n",
    "\n",
    "print(\"üìà Analysis Results:\\n\")\n",
    "for plot in plots:\n",
    "    plot_path = os.path.join(analysis_dir, plot)\n",
    "    if os.path.exists(plot_path):\n",
    "        print(f\"## {plot.replace('_', ' ').title()}\")\n",
    "        display(Image(plot_path))\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Plot not found: {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations from analysis results\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Add project directory to Python path (if not already added)\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "results_path = \"data/processed/analysis/compression_analysis.csv\"\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    print(\"üìä Generating visualizations...\")\n",
    "    !python scripts/visualize_results.py \\\n",
    "        --results {results_path} \\\n",
    "        --output-dir data/processed/analysis \\\n",
    "        --plot all\n",
    "    print(\"\\n‚úÖ Visualizations complete!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Results file not found: {results_path}\")\n",
    "    print(\"Please run the compression analysis step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained and compressed models to Google Drive\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Note: Google Drive already mounted in Cell 2\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "\n",
    "# Models to save (preserving all timestamped versions)\n",
    "OUTPUT_FILES = [\n",
    "    \"src/models/global_model.h5\",  # Latest baseline model\n",
    "]\n",
    "\n",
    "# Find all timestamped models\n",
    "models_dir = Path(PROJECT_DIR) / \"src\" / \"models\"\n",
    "if models_dir.exists():\n",
    "    timestamped_models = list(models_dir.glob(\"global_model_*.h5\"))\n",
    "    if timestamped_models:\n",
    "        OUTPUT_FILES.extend([f\"src/models/{f.name}\" for f in timestamped_models])\n",
    "        print(f\"üì¶ Found {len(timestamped_models)} timestamped model(s)\")\n",
    "\n",
    "dest_dir = \"/content/drive/MyDrive/TinyML_models\"\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "# Create directory structure in Drive\n",
    "drive_src_dir = os.path.join(dest_dir, \"src\", \"models\")\n",
    "os.makedirs(drive_src_dir, exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving models to Google Drive...\\n\")\n",
    "found_any = False\n",
    "for fname in OUTPUT_FILES:\n",
    "    src_path = os.path.join(PROJECT_DIR, fname)\n",
    "    if os.path.exists(src_path):\n",
    "        # Preserve directory structure\n",
    "        dst_path = os.path.join(dest_dir, fname)\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        print(f\"‚úÖ Saved: {fname}\")\n",
    "        found_any = True\n",
    "\n",
    "if not found_any:\n",
    "    print(\"‚ö†Ô∏è No model files found. Make sure training completed successfully.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All models saved to: {dest_dir}/src/models/\")\n",
    "    print(\"   üìå Timestamped versions preserved\")\n",
    "    print(\"   üìå Latest: global_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
