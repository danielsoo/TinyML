{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38be2eb3",
      "metadata": {},
      "source": [
        "# üß† TinyML - Colab Training Notebook\n",
        "\n",
        "**Version:** V.1.1.11 (Check commit info below after GitHub update)\n",
        "\n",
        "This notebook trains your TinyML model on Google Colab GPU.\n",
        "\n",
        "**Flow:**\n",
        "1. Check GPU\n",
        "2. Clone or update your GitHub repo\n",
        "3. Install Colab-compatible dependencies\n",
        "4. Run training script (`scripts/train.py`) ‚Üí saves `src/models/global_model.h5`\n",
        "5. Run compression pipeline (`compression.py`) ‚Üí Distillation ‚Üí Pruning ‚Üí Quantization ‚Üí TFLite\n",
        "6. Save exported models to Google Drive\n",
        "\n",
        "**Workflow:**\n",
        "- **Training**: `scripts/train.py` (uses `config/federated_colab.yaml`)\n",
        "- **Compression**: `compression.py` (full pipeline with all compression techniques)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8b4a95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å Ïó∞Í≤∞\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Ìè¥Îçî Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "DATA_DIR = \"/content/drive/MyDrive/TinyML_models\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Runtime & GPU check\n",
        "Make sure you set **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU** before running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi || echo \"No NVIDIA GPU detected. Please enable GPU in Runtime settings.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Clone or update TinyML repository\n",
        "\n",
        "Set your GitHub repo URL if different.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0d76aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "REPO_URL = \"https://github.com/danielsoo/TinyML.git\"  # change if needed\n",
        "PROJECT_DIR = \"/content/TinyML\"\n",
        "\n",
        "print(\"üîÑ Updating repository from GitHub...\")\n",
        "if not os.path.exists(PROJECT_DIR):\n",
        "    print(f\"üì• Cloning repository from {REPO_URL}...\")\n",
        "    result = subprocess.run([\"git\", \"clone\", REPO_URL, PROJECT_DIR], \n",
        "                          capture_output=True, text=True, check=True)\n",
        "    print(\"‚úÖ Repository cloned successfully\")\n",
        "else:\n",
        "    print(f\"üì• Pulling latest changes from {REPO_URL}...\")\n",
        "    # Change to project directory and pull\n",
        "    os.chdir(PROJECT_DIR)\n",
        "    \n",
        "    # Get current branch name\n",
        "    branch_result = subprocess.run([\"git\", \"branch\", \"--show-current\"], \n",
        "                                   capture_output=True, text=True, check=False)\n",
        "    current_branch = branch_result.stdout.strip() if branch_result.returncode == 0 else \"main\"\n",
        "    if not current_branch:\n",
        "        current_branch = \"main\"\n",
        "    \n",
        "    print(f\"   Current branch: {current_branch}\")\n",
        "    \n",
        "    # Try test branch first (where files are), then current branch, then main\n",
        "    # Local uses 'test' branch, so prioritize it\n",
        "    branches_to_try = [\"test\"]\n",
        "    if current_branch not in branches_to_try:\n",
        "        branches_to_try.append(current_branch)\n",
        "    if \"main\" not in branches_to_try:\n",
        "        branches_to_try.append(\"main\")\n",
        "    \n",
        "    # Fetch latest changes from all branches\n",
        "    fetch_result = subprocess.run([\"git\", \"fetch\", \"origin\", \"--all\"], \n",
        "                                 capture_output=True, text=True, check=False)\n",
        "    if fetch_result.returncode != 0:\n",
        "        print(f\"‚ö†Ô∏è  Git fetch had issues: {fetch_result.stderr[:200]}\")\n",
        "    \n",
        "    # Try to checkout and pull from the branch\n",
        "    updated = False\n",
        "    for branch in branches_to_try:\n",
        "        print(f\"\\n   Trying to update from branch: {branch}\")\n",
        "        \n",
        "        # Checkout the branch\n",
        "        checkout_result = subprocess.run([\"git\", \"checkout\", branch], \n",
        "                                        capture_output=True, text=True, check=False)\n",
        "        \n",
        "        # Pull latest changes\n",
        "        pull_result = subprocess.run([\"git\", \"pull\", \"origin\", branch], \n",
        "                                    capture_output=True, text=True, check=False)\n",
        "        \n",
        "        if pull_result.returncode == 0:\n",
        "            print(f\"   ‚úÖ Successfully updated from {branch}\")\n",
        "            if pull_result.stdout.strip() and \"Already up to date\" not in pull_result.stdout:\n",
        "                print(f\"   Changes: {pull_result.stdout.strip()[:150]}\")\n",
        "            updated = True\n",
        "            break\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Could not update from {branch}: {pull_result.stderr[:100]}\")\n",
        "    \n",
        "    if not updated:\n",
        "        print(f\"‚ö†Ô∏è  Could not update from any branch, continuing with current state...\")\n",
        "    \n",
        "    # Clean up old modelcompression folder if it exists (matching local structure)\n",
        "    old_compression_dir = Path(PROJECT_DIR) / \"src\" / \"modelcompression\"\n",
        "    if old_compression_dir.exists():\n",
        "        print(f\"\\nüßπ Cleaning up old folder structure...\")\n",
        "        print(f\"   Removing: {old_compression_dir}\")\n",
        "        import shutil\n",
        "        shutil.rmtree(old_compression_dir)\n",
        "        print(f\"   ‚úÖ Removed old modelcompression folder\")\n",
        "    \n",
        "    # Verify critical files exist after update\n",
        "    compression_dir = Path(PROJECT_DIR) / \"src\" / \"compression\"\n",
        "    required_files = [\"distillation.py\", \"pruning.py\", \"quantization.py\"]\n",
        "    \n",
        "    if compression_dir.exists():\n",
        "        files_in_dir = [f.name for f in compression_dir.iterdir() if f.is_file()]\n",
        "        missing = [f for f in required_files if f not in files_in_dir]\n",
        "        if missing:\n",
        "            print(f\"\\n‚ö†Ô∏è  WARNING: After GitHub update, files still missing: {missing}\")\n",
        "            print(f\"   Files in {compression_dir}: {files_in_dir}\")\n",
        "            print(f\"   Current branch: {current_branch}\")\n",
        "            print(f\"   Please ensure files are committed and pushed to the 'test' branch.\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: {compression_dir} does not exist after GitHub update!\")\n",
        "        print(f\"   This indicates the folder structure may not be in the current branch.\")\n",
        "\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# Show current commit information\n",
        "print(\"\\nüìå Repository Version Info:\")\n",
        "commit_hash_result = subprocess.run([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], \n",
        "                                    capture_output=True, text=True, check=False)\n",
        "commit_msg_result = subprocess.run([\"git\", \"log\", \"-1\", \"--pretty=format:%s\"], \n",
        "                                   capture_output=True, text=True, check=False)\n",
        "commit_date_result = subprocess.run([\"git\", \"log\", \"-1\", \"--pretty=format:%ad\", \"--date=short\"], \n",
        "                                    capture_output=True, text=True, check=False)\n",
        "\n",
        "if commit_hash_result.returncode == 0:\n",
        "    commit_hash = commit_hash_result.stdout.strip()\n",
        "    commit_msg = commit_msg_result.stdout.strip() if commit_msg_result.returncode == 0 else \"N/A\"\n",
        "    commit_date = commit_date_result.stdout.strip() if commit_date_result.returncode == 0 else \"N/A\"\n",
        "    print(f\"   Commit Hash: {commit_hash}\")\n",
        "    print(f\"   Commit Message: {commit_msg}\")\n",
        "    print(f\"   Commit Date: {commit_date}\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  Could not retrieve commit information\")\n",
        "\n",
        "# Add project directory to Python path for module imports\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "print(f\"\\n‚úÖ Project directory ready: {PROJECT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f4756a1",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£.5 Update config data path\n",
        "Google DriveÏóê ÏûàÎäî Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú(`DATA_DIR`)Î•º `config/federated_colab.yaml`Ïóê Î∞òÏòÅÌï©ÎãàÎã§.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "231529f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "config_path = Path(PROJECT_DIR) / \"config\" / \"federated_colab.yaml\"\n",
        "\n",
        "if config_path.exists():\n",
        "    with config_path.open(\"r\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "\n",
        "    cfg.setdefault(\"data\", {})\n",
        "    cfg[\"data\"][\"path\"] = DATA_DIR\n",
        "\n",
        "    with config_path.open(\"w\") as f:\n",
        "        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
        "\n",
        "    print(f\"Updated federated_colab.yaml data.path -> {cfg['data']['path']}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Cannot find {config_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Generate Colab-specific requirements (no macOS-only packages)\n",
        "\n",
        "We remove `tensorflow-macos` and `tensorflow-metal` from `requirements.txt` automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "src_req = \"requirements.txt\"\n",
        "colab_req = \"colab_requirements.txt\"\n",
        "\n",
        "# Skip packages that are preinstalled or incompatible on Colab (tensorflow/numpy variants)\n",
        "skip_keywords = [\"tensorflow\", \"numpy\", \"tensorflow-macos\", \"tensorflow-metal\"]\n",
        "\n",
        "if os.path.exists(src_req):\n",
        "    with open(src_req, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    with open(colab_req, \"w\") as f:\n",
        "        for line in lines:\n",
        "            if any(kw in line for kw in skip_keywords):\n",
        "                continue\n",
        "            f.write(line)\n",
        "\n",
        "    print(\"Generated:\", colab_req)\n",
        "    with open(colab_req, \"r\") as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(\"No requirements.txt found. Skipping Colab requirements generation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Install dependencies (Colab compatible)\n",
        "\n",
        "- Installs from `colab_requirements.txt` if present.\n",
        "- Installs standard `tensorflow` for Linux GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "\n",
        "if os.path.exists(\"colab_requirements.txt\"):\n",
        "    print(\"   Installing from colab_requirements.txt...\")\n",
        "    subprocess.run([\"pip\", \"install\", \"-r\", \"colab_requirements.txt\"], check=True)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  colab_requirements.txt not found. Install your packages manually if needed.\")\n",
        "\n",
        "print(\"   Installing Flower...\")\n",
        "subprocess.run([\"pip\", \"install\", \"flwr[simulation]\"], check=True)\n",
        "\n",
        "# Fix protobuf compatibility issue with TensorFlow 2.19.0\n",
        "# TensorFlow 2.19.0 requires protobuf==3.20.3, but Colab may have newer version\n",
        "print(\"\\nüîß Fixing protobuf compatibility (TensorFlow 2.19.0 requires protobuf==3.20.3)...\")\n",
        "subprocess.run([\"pip\", \"install\", \"--force-reinstall\", \"protobuf==3.20.3\"], check=True)\n",
        "print(\"‚úÖ Protobuf fixed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix protobuf compatibility issue (if needed)\n",
        "# If you see AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
        "# Uncomment the following line:\n",
        "# !pip install protobuf==3.20.3\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Check protobuf version\n",
        "try:\n",
        "    import google.protobuf\n",
        "    print(\"Protobuf version:\", google.protobuf.__version__)\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ (Optional) Download or prepare dataset\n",
        "\n",
        "Edit this cell if your training script expects data in a specific path.\n",
        "For example, you can mount Google Drive or download from Kaggle here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: mount Google Drive if your data is stored there.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Example: create a data directory\n",
        "# os.makedirs('data', exist_ok=True)\n",
        "# Then copy or download your dataset into ./data\n",
        "\n",
        "print(\"Dataset preparation step: customize as needed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Run training\n",
        "\n",
        "This cell tries to run `train.py` at repo root.\n",
        "If your main script is at a different path, edit accordingly (e.g. `src/train.py`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe73708",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"/content/drive/MyDrive/TinyML_models\")  # CSVÎì§Ïù¥ ÏûàÎäî Í≤ΩÎ°úÎ°ú ÏàòÏ†ï\n",
        "csv_paths = sorted(data_dir.glob(\"*.csv\"))\n",
        "\n",
        "dfs = [pd.read_csv(p, low_memory=False) for p in csv_paths]\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "print(\"Total Samples:\", len(df))\n",
        "print(df[\"attack\"].value_counts())\n",
        "print(df[\"attack\"].value_counts(normalize=True))  # ÎπÑÏú® ÌôïÏù∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413afbf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "PROJECT_DIR = \"/content/TinyML\"\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# Add project directory to Python path\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "# Use train.py script (unified training script)\n",
        "# This will automatically detect Colab environment and use federated_colab.yaml\n",
        "print(\"üöÄ Running training with scripts/train.py...\")\n",
        "print(\"   This will use config/federated_colab.yaml automatically\\n\")\n",
        "\n",
        "import subprocess\n",
        "result = subprocess.run(\n",
        "    [sys.executable, \"scripts/train.py\", \"--config\", \"config/federated_colab.yaml\"],\n",
        "    cwd=PROJECT_DIR,\n",
        "    check=False\n",
        ")\n",
        "\n",
        "# Check if model was actually saved\n",
        "model_path = Path(PROJECT_DIR) / \"src\" / \"models\" / \"global_model.h5\"\n",
        "model_exists = model_path.exists() and model_path.stat().st_size > 0\n",
        "\n",
        "if result.returncode == 0 or model_exists:\n",
        "    print(\"\\n‚úÖ Training complete!\")\n",
        "    print(\"   Model saved to: src/models/global_model.h5\")\n",
        "    if result.returncode != 0:\n",
        "        print(\"   ‚ö†Ô∏è  Note: Training process returned non-zero exit code, but model was saved.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Training failed!\")\n",
        "    print(f\"   Exit code: {result.returncode}\")\n",
        "    print(\"   Model file not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f694b385",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£.5 Compression Pipeline (Full Pipeline)\n",
        "\n",
        "Run the complete compression pipeline: Distillation ‚Üí Pruning ‚Üí Quantization ‚Üí TFLite Export.\n",
        "\n",
        "This uses the trained model from the previous step and applies all compression techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c722270f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run full compression pipeline\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_DIR = \"/content/TinyML\"\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# Add project directory to Python path\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "# Ensure __init__.py exists in src/compression/ directory\n",
        "compression_init = Path(PROJECT_DIR) / \"src\" / \"compression\" / \"__init__.py\"\n",
        "if not compression_init.exists():\n",
        "    print(\"üìù Creating src/compression/__init__.py...\")\n",
        "    compression_init.parent.mkdir(parents=True, exist_ok=True)\n",
        "    compression_init.write_text('\"\"\"\\nCompression module for TinyML model compression techniques.\\n\\nThis module provides:\\n- Knowledge Distillation\\n- Structured Pruning\\n- Quantization\\n\"\"\"\\n')\n",
        "    print(\"‚úÖ Created __init__.py\\n\")\n",
        "\n",
        "# Check if model exists\n",
        "model_path = \"src/models/global_model.h5\"\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"‚ö†Ô∏è  WARNING: No trained model found!\")\n",
        "    print(\"‚ö†Ô∏è  Please run the training step (Cell 17) first.\")\n",
        "    print(\"‚ö†Ô∏è  The compression script will train a quick test model, but results will be less accurate.\")\n",
        "    print()\n",
        "\n",
        "# Run compression pipeline\n",
        "print(\"üöÄ Running compression pipeline...\")\n",
        "print(\"   This will run:\")\n",
        "print(\"   1. Test 1: Full pipeline (Train ‚Üí Distillation ‚Üí Pruning ‚Üí Quantization ‚Üí TFLite)\")\n",
        "print(\"   2. Test 2: Saved model compression (Load ‚Üí Prune ‚Üí Quantize ‚Üí TFLite)\")\n",
        "print()\n",
        "\n",
        "# Run compression.py - Ensure proper path setup (matching local structure)\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# CRITICAL: Ensure project root is at the BEGINNING of sys.path\n",
        "if PROJECT_DIR in sys.path:\n",
        "    sys.path.remove(PROJECT_DIR)\n",
        "sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "# Expected structure (matching local):\n",
        "# /content/TinyML/\n",
        "#   compression.py (root)\n",
        "#   src/\n",
        "#     compression/\n",
        "#       __init__.py\n",
        "#       distillation.py\n",
        "#       pruning.py\n",
        "#       quantization.py\n",
        "\n",
        "compression_dir = Path(PROJECT_DIR) / \"src\" / \"compression\"\n",
        "required_files = [\"distillation.py\", \"pruning.py\", \"quantization.py\"]\n",
        "\n",
        "print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
        "print(f\"üìÅ Project directory: {PROJECT_DIR}\")\n",
        "print(f\"üìÅ Expected compression dir: {compression_dir}\")\n",
        "print(f\"üìÅ src/compression exists: {compression_dir.exists()}\")\n",
        "\n",
        "# Verify structure (matching local exactly)\n",
        "if not compression_dir.exists():\n",
        "    print(f\"‚ùå ERROR: {compression_dir} not found!\")\n",
        "    print(f\"   Please ensure GitHub repository is up to date.\")\n",
        "    raise FileNotFoundError(f\"Compression directory not found: {compression_dir}\")\n",
        "\n",
        "# Check required files\n",
        "files_in_dir = [f.name for f in compression_dir.iterdir() if f.is_file()]\n",
        "print(f\"üìÅ Files in src/compression: {files_in_dir}\")\n",
        "\n",
        "missing_files = [f for f in required_files if f not in files_in_dir]\n",
        "if missing_files:\n",
        "    print(f\"\\n‚ùå ERROR: Missing required files: {missing_files}\")\n",
        "    print(f\"   Please ensure GitHub repository is up to date.\")\n",
        "    print(f\"   Expected files in {compression_dir}:\")\n",
        "    for req_file in required_files:\n",
        "        status = \"‚úÖ\" if req_file in files_in_dir else \"‚ùå\"\n",
        "        print(f\"   {status} {req_file}\")\n",
        "    raise FileNotFoundError(f\"Missing files: {missing_files}\")\n",
        "\n",
        "# Ensure __init__.py exists\n",
        "init_file = compression_dir / \"__init__.py\"\n",
        "if not init_file.exists():\n",
        "    print(f\"üìù Creating {init_file.name}...\")\n",
        "    init_file.write_text('\"\"\"\\nCompression module for TinyML model compression techniques.\\n\\nThis module provides:\\n- Knowledge Distillation\\n- Structured Pruning\\n- Quantization\\n\"\"\"\\n')\n",
        "\n",
        "# Verify compression.py exists at root\n",
        "compression_script = Path(PROJECT_DIR) / \"compression.py\"\n",
        "if not compression_script.exists():\n",
        "    print(f\"‚ùå ERROR: compression.py not found at {compression_script}\")\n",
        "    print(f\"   This file should be at the project root.\")\n",
        "    raise FileNotFoundError(f\"compression.py not found: {compression_script}\")\n",
        "\n",
        "# Test imports before running\n",
        "print(f\"\\nüîç Testing imports...\")\n",
        "try:\n",
        "    import src.compression.distillation\n",
        "    import src.compression.pruning\n",
        "    import src.compression.quantization\n",
        "    print(\"‚úÖ Successfully imported all compression modules\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import compression modules: {e}\")\n",
        "    print(f\"   sys.path (first 5): {sys.path[:5]}\")\n",
        "    print(f\"   compression_dir: {compression_dir}\")\n",
        "    raise\n",
        "\n",
        "# Run compression.py\n",
        "print(f\"\\nüöÄ Running compression.py...\")\n",
        "try:\n",
        "    import runpy\n",
        "    original_argv = sys.argv.copy()\n",
        "    sys.argv = [\"compression.py\"]\n",
        "    \n",
        "    # Run compression.py\n",
        "    runpy.run_path(str(compression_script), run_name=\"__main__\")\n",
        "    \n",
        "    sys.argv = original_argv\n",
        "    print(\"\\n‚úÖ Compression pipeline complete!\")\n",
        "    print(\"   Check outputs/test_pipeline/ and models/tflite/ for compressed models\")\n",
        "            \n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    if 'original_argv' in locals():\n",
        "        sys.argv = original_argv\n",
        "    print(\"\\n‚ùå Compression pipeline failed with error:\")\n",
        "    print(f\"   {type(e).__name__}: {e}\")\n",
        "    print(\"\\nFull traceback:\")\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "717b8a0a",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£.6 FGSM Adversarial Attack Testing\n",
        "\n",
        "Test FGSM (Fast Gradient Sign Method) attack on the trained and compressed models.\n",
        "This evaluates the model's robustness against adversarial examples.\n",
        "\n",
        "**Note:** This can be run on both the original model and compressed models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa91c39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run FGSM attack testing\n",
        "import os\n",
        "import sys\n",
        "\n",
        "PROJECT_DIR = \"/content/TinyML\"\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# Add project directory to Python path\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "# Check if model exists\n",
        "model_path = \"src/models/global_model.h5\"\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"‚ö†Ô∏è  WARNING: No trained model found!\")\n",
        "    print(\"‚ö†Ô∏è  Please run the training step (Cell 17) first.\")\n",
        "    print(\"‚ö†Ô∏è  The script will train a quick test model, but results will be less accurate.\")\n",
        "    print()\n",
        "\n",
        "# Run FGSM attack test\n",
        "print(\"Running FGSM attack testing...\")\n",
        "print(\"This may take a few minutes depending on dataset size.\\n\")\n",
        "\n",
        "!python scripts/test_fgsm_attack.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Save trained model(s) to Google Drive\n",
        "\n",
        "This will look for common output filenames (e.g. `tiny_model.tflite`) in the project root and copy them to your Drive.\n",
        "Edit `OUTPUT_FILES` if your script uses different names or locations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2516561b",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Compression Analysis\n",
        "\n",
        "Analyze model size, accuracy, and inference speed at each compression stage.\n",
        "Generate visualizations and reports.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10834c39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export trained model to TFLite (optional, for comparison)\n",
        "import tensorflow as tf\n",
        "import yaml\n",
        "\n",
        "# Load config\n",
        "with open(\"config/federated_colab.yaml\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# Load trained model\n",
        "model_path = \"src/models/global_model.h5\"\n",
        "if os.path.exists(model_path):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    \n",
        "    # Export to TFLite\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    tflite_model = converter.convert()\n",
        "    \n",
        "    # Save\n",
        "    tflite_path = \"src/models/global_model.tflite\"\n",
        "    with open(tflite_path, \"wb\") as f:\n",
        "        f.write(tflite_model)\n",
        "    print(f\"‚úÖ Saved TFLite model: {tflite_path}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Model not found: {model_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eadfa898",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run compression analysis\n",
        "import os\n",
        "import sys\n",
        "\n",
        "PROJECT_DIR = \"/content/TinyML\"\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# Add project directory to Python path\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "# Analyze models - Îëê Î™®Îç∏Ïùò ÏïïÏ∂ï Í≤∞Í≥º Î™®Îëê Ìè¨Ìï®\n",
        "models_to_analyze = []\n",
        "\n",
        "# 1. Train.pyÎ°ú ÌïôÏäµÌïú Î™®Îç∏ (Federated Learning)\n",
        "# Original model\n",
        "if os.path.exists(\"src/models/global_model.h5\"):\n",
        "    models_to_analyze.append(\"Federated_Original:src/models/global_model.h5\")\n",
        "\n",
        "if os.path.exists(\"src/models/global_model.tflite\"):\n",
        "    models_to_analyze.append(\"Federated_Original_TFLite:src/models/global_model.tflite\")\n",
        "\n",
        "# Compression.pyÏóêÏÑú ÏïïÏ∂ïÌïú train.py Î™®Îç∏\n",
        "if os.path.exists(\"models/tflite/saved_model_original.tflite\"):\n",
        "    models_to_analyze.append(\"Federated_Compressed_Original:models/tflite/saved_model_original.tflite\")\n",
        "\n",
        "if os.path.exists(\"models/tflite/saved_model_pruned_quantized.tflite\"):\n",
        "    models_to_analyze.append(\"Federated_Compressed_PrunedQuantized:models/tflite/saved_model_pruned_quantized.tflite\")\n",
        "\n",
        "# 2. Compression.py ÏûêÏ≤¥ ÌïôÏäµ Î™®Îç∏ (Self-Trained)\n",
        "# Compression pipelineÏóêÏÑú ÏÉùÏÑ±Îêú Î™®Îç∏Îì§\n",
        "if os.path.exists(\"outputs/test_pipeline/original_float32.tflite\"):\n",
        "    models_to_analyze.append(\"SelfTrained_Original:outputs/test_pipeline/original_float32.tflite\")\n",
        "\n",
        "if os.path.exists(\"outputs/test_pipeline/pruned_float32.tflite\"):\n",
        "    models_to_analyze.append(\"SelfTrained_Pruned:outputs/test_pipeline/pruned_float32.tflite\")\n",
        "\n",
        "if os.path.exists(\"outputs/test_pipeline/pruned_quantized_int8.tflite\"):\n",
        "    models_to_analyze.append(\"SelfTrained_PrunedQuantized:outputs/test_pipeline/pruned_quantized_int8.tflite\")\n",
        "\n",
        "if not models_to_analyze:\n",
        "    print(\"‚ö†Ô∏è  No models found for analysis. Make sure:\")\n",
        "    print(\"   1. Training step (Cell 17) completed\")\n",
        "    print(\"   2. Compression pipeline (Cell 21) completed\")\n",
        "    print()\n",
        "else:\n",
        "    print(f\"üìä Found {len(models_to_analyze)} model(s) to analyze:\")\n",
        "    for model in models_to_analyze:\n",
        "        print(f\"   - {model}\")\n",
        "    print()\n",
        "\n",
        "    models_str = \" \".join([f'\"{m}\"' for m in models_to_analyze])\n",
        "\n",
        "    # Use first original model as baseline\n",
        "    baseline = None\n",
        "    if os.path.exists(\"src/models/global_model.h5\"):\n",
        "        baseline = \"src/models/global_model.h5\"\n",
        "    elif os.path.exists(\"outputs/test_pipeline/original_float32.tflite\"):\n",
        "        baseline = \"outputs/test_pipeline/original_float32.tflite\"\n",
        "\n",
        "    cmd = f\"\"\"python scripts/analyze_compression.py \\\n",
        "        --models {models_str} \\\n",
        "        --config config/federated_colab.yaml \\\n",
        "        --output-dir data/processed/analysis \\\n",
        "        --format all\"\"\"\n",
        "\n",
        "    if baseline:\n",
        "        cmd += f\" \\\\\\n        --baseline {baseline}\"\n",
        "\n",
        "    print(\"Running compression analysis...\")\n",
        "    print(f\"Command: {cmd}\\n\")\n",
        "    print(f\"Python path: {sys.path[:3]}...\\n\")\n",
        "    !{cmd}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e86b54af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate visualizations\n",
        "import os\n",
        "import sys\n",
        "\n",
        "PROJECT_DIR = \"/content/TinyML\"\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# Add project directory to Python path (if not already added)\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "results_path = \"data/processed/analysis/compression_analysis.csv\"\n",
        "\n",
        "if os.path.exists(results_path):\n",
        "    print(\"Generating visualizations...\")\n",
        "    !python scripts/visualize_results.py \\\n",
        "        --results {results_path} \\\n",
        "        --output-dir data/processed/analysis \\\n",
        "        --plot all\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Results file not found: {results_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47488b8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display visualizations inline\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "analysis_dir = \"data/processed/analysis\"\n",
        "plots = [\n",
        "    \"size_vs_accuracy.png\",\n",
        "    \"compression_metrics.png\",\n",
        "    \"compression_ratio.png\"\n",
        "]\n",
        "\n",
        "for plot in plots:\n",
        "    plot_path = os.path.join(analysis_dir, plot)\n",
        "    if os.path.exists(plot_path):\n",
        "        print(f\"\\n## {plot}\")\n",
        "        display(Image(plot_path))\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Plot not found: {plot_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "776cca6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy analysis results to Google Drive\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "analysis_dir = \"data/processed/analysis\"\n",
        "drive_dir = \"/content/drive/MyDrive/TinyML_models/analysis\"\n",
        "\n",
        "# Create directory\n",
        "os.makedirs(drive_dir, exist_ok=True)\n",
        "\n",
        "# Copy all analysis files\n",
        "found_any = False\n",
        "if os.path.exists(analysis_dir):\n",
        "    for file in os.listdir(analysis_dir):\n",
        "        src = os.path.join(analysis_dir, file)\n",
        "        dst = os.path.join(drive_dir, file)\n",
        "        if os.path.isfile(src):\n",
        "            shutil.copy(src, dst)\n",
        "            print(f\"‚úÖ Copied: {file}\")\n",
        "            found_any = True\n",
        "\n",
        "if found_any:\n",
        "    print(f\"\\n‚úÖ All analysis results saved to: {drive_dir}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è No analysis files found in {analysis_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de21ae6",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£.5 Save Compression Pipeline Outputs to Google Drive\n",
        "\n",
        "Copy compressed models from compression.py to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db59b38",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy compression pipeline outputs to Google Drive\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_DIR = \"/content/TinyML\"\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dest_dir = \"/content/drive/MyDrive/TinyML_models\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Compression pipeline outputs\n",
        "compression_dirs = [\n",
        "    \"outputs/test_pipeline\",\n",
        "    \"models/tflite\"\n",
        "]\n",
        "\n",
        "found_any = False\n",
        "for comp_dir in compression_dirs:\n",
        "    src_dir = os.path.join(PROJECT_DIR, comp_dir)\n",
        "    if os.path.exists(src_dir):\n",
        "        dst_dir = os.path.join(dest_dir, comp_dir)\n",
        "        os.makedirs(dst_dir, exist_ok=True)\n",
        "        \n",
        "        # Copy all files in the directory\n",
        "        for file in os.listdir(src_dir):\n",
        "            src_path = os.path.join(src_dir, file)\n",
        "            dst_path = os.path.join(dst_dir, file)\n",
        "            if os.path.isfile(src_path):\n",
        "                shutil.copy(src_path, dst_path)\n",
        "                print(f\"‚úÖ Copied {comp_dir}/{file}\")\n",
        "                found_any = True\n",
        "\n",
        "if found_any:\n",
        "    print(f\"\\n‚úÖ All compression outputs saved to: {dest_dir}/\")\n",
        "    print(\"   üìå Compressed models: outputs/test_pipeline/ and models/tflite/\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No compression outputs found. Make sure compression.py (Cell 21) ran successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_DIR = \"/content/TinyML\"\n",
        "OUTPUT_FILES = [\n",
        "    \"src/models/global_model.h5\",  # Latest model (always copied)\n",
        "    \"src/models/global_model.tflite\",\n",
        "    \"models/global_model.h5\",\n",
        "    \"models/global_model.tflite\",\n",
        "    \"tiny_model.tflite\",\n",
        "    \"model.tflite\",\n",
        "    \"model.h5\",\n",
        "    \"saved_model.pb\"\n",
        "]\n",
        "\n",
        "# Also find all timestamped models to preserve history\n",
        "models_dir = Path(PROJECT_DIR) / \"src\" / \"models\"\n",
        "if models_dir.exists():\n",
        "    timestamped_models = list(models_dir.glob(\"global_model_*.h5\"))\n",
        "    if timestamped_models:\n",
        "        OUTPUT_FILES.extend([f\"src/models/{f.name}\" for f in timestamped_models])\n",
        "        print(f\"üì¶ Found {len(timestamped_models)} timestamped model(s) to preserve\")\n",
        "\n",
        "# Mount Drive to store the trained models\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dest_dir = \"/content/drive/MyDrive/TinyML_models\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Create src/models subdirectory in Drive to preserve structure\n",
        "drive_src_dir = os.path.join(dest_dir, \"src\", \"models\")\n",
        "os.makedirs(drive_src_dir, exist_ok=True)\n",
        "\n",
        "found_any = False\n",
        "for fname in OUTPUT_FILES:\n",
        "    src_path = os.path.join(PROJECT_DIR, fname)\n",
        "    if os.path.exists(src_path):\n",
        "        # Keep directory structure in Drive\n",
        "        dst_path = os.path.join(dest_dir, fname)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "        print(f\"‚úÖ Copied {fname}\")\n",
        "        found_any = True\n",
        "\n",
        "if not found_any:\n",
        "    print(\"‚ö†Ô∏è No known model files found. Make sure your training script saves a model and update OUTPUT_FILES if needed.\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ All models saved to: {dest_dir}/src/models/\")\n",
        "    print(\"   üìå Timestamped models are preserved (no overwriting)\")\n",
        "    print(\"   üìå Latest model: global_model.h5\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
