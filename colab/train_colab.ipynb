{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† TinyML - Colab Training Notebook\n",
    "\n",
    "This notebook trains your TinyML model on Google Colab GPU.\n",
    "\n",
    "**Flow:**\n",
    "1. Check GPU\n",
    "2. Clone or update your GitHub repo\n",
    "3. Install Colab-compatible dependencies\n",
    "4. Run training script (`train.py` or `src/train.py`)\n",
    "5. Save exported model (e.g. `tiny_model.tflite`) to Google Drive\n",
    "\n",
    "Customize the training script path in the **Run training** cell if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å Ïó∞Í≤∞\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ Ìè¥Îçî Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "DATA_DIR = \"/content/drive/MyDrive/TinyML_models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Runtime & GPU check\n",
    "Make sure you set **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU** before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi || echo \"No NVIDIA GPU detected. Please enable GPU in Runtime settings.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone or update TinyML repository\n",
    "\n",
    "Set your GitHub repo URL if different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/danielsoo/TinyML.git\"  # change if needed\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "\n",
    "print(\"üîÑ Updating repository from GitHub...\")\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print(f\"üì• Cloning repository from {REPO_URL}...\")\n",
    "    result = subprocess.run([\"git\", \"clone\", REPO_URL, PROJECT_DIR], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    print(\"‚úÖ Repository cloned successfully\")\n",
    "else:\n",
    "    print(f\"üì• Pulling latest changes from {REPO_URL}...\")\n",
    "    # Change to project directory and pull\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    # Fetch latest changes\n",
    "    subprocess.run([\"git\", \"fetch\", \"origin\"], \n",
    "                  capture_output=True, text=True, check=False)\n",
    "    # Pull latest changes\n",
    "    result = subprocess.run([\"git\", \"pull\", \"origin\", \"main\"], \n",
    "                          capture_output=True, text=True, check=False)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Repository updated successfully\")\n",
    "        if result.stdout.strip():\n",
    "            print(f\"   Changes: {result.stdout.strip()[:100]}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Git pull had issues (may be up to date): {result.stderr[:100]}\")\n",
    "        # Try to continue anyway\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Show current commit\n",
    "commit_result = subprocess.run([\"git\", \"log\", \"-1\", \"--oneline\"], \n",
    "                              capture_output=True, text=True, check=False)\n",
    "if commit_result.returncode == 0:\n",
    "    print(f\"üìå Current commit: {commit_result.stdout.strip()}\")\n",
    "\n",
    "# Add project directory to Python path for module imports\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "print(f\"\\n‚úÖ Project directory ready: {PROJECT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4756a1",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£.5 Update config data path\n",
    "Google DriveÏóê ÏûàÎäî Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú(`DATA_DIR`)Î•º `config/federated_colab.yaml`Ïóê Î∞òÏòÅÌï©ÎãàÎã§.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231529f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "config_path = Path(PROJECT_DIR) / \"config\" / \"federated_colab.yaml\"\n",
    "\n",
    "if config_path.exists():\n",
    "    with config_path.open(\"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    cfg.setdefault(\"data\", {})\n",
    "    cfg[\"data\"][\"path\"] = DATA_DIR\n",
    "\n",
    "    with config_path.open(\"w\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "    print(f\"Updated federated_colab.yaml data.path -> {cfg['data']['path']}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cannot find {config_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Generate Colab-specific requirements (no macOS-only packages)\n",
    "\n",
    "We remove `tensorflow-macos` and `tensorflow-metal` from `requirements.txt` automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "src_req = \"requirements.txt\"\n",
    "colab_req = \"colab_requirements.txt\"\n",
    "\n",
    "# Skip packages that are preinstalled or incompatible on Colab (tensorflow/numpy variants)\n",
    "skip_keywords = [\"tensorflow\", \"numpy\", \"tensorflow-macos\", \"tensorflow-metal\"]\n",
    "\n",
    "if os.path.exists(src_req):\n",
    "    with open(src_req, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with open(colab_req, \"w\") as f:\n",
    "        for line in lines:\n",
    "            if any(kw in line for kw in skip_keywords):\n",
    "                continue\n",
    "            f.write(line)\n",
    "\n",
    "    print(\"Generated:\", colab_req)\n",
    "    with open(colab_req, \"r\") as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"No requirements.txt found. Skipping Colab requirements generation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Install dependencies (Colab compatible)\n",
    "\n",
    "- Installs from `colab_requirements.txt` if present.\n",
    "- Installs standard `tensorflow` for Linux GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "if os.path.exists(\"colab_requirements.txt\"):\n",
    "    print(\"   Installing from colab_requirements.txt...\")\n",
    "    subprocess.run([\"pip\", \"install\", \"-r\", \"colab_requirements.txt\"], check=True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  colab_requirements.txt not found. Install your packages manually if needed.\")\n",
    "\n",
    "print(\"   Installing Flower...\")\n",
    "subprocess.run([\"pip\", \"install\", \"flwr[simulation]\"], check=True)\n",
    "\n",
    "# Fix protobuf compatibility issue with TensorFlow 2.19.0\n",
    "# TensorFlow 2.19.0 requires protobuf==3.20.3, but Colab may have newer version\n",
    "print(\"\\nüîß Fixing protobuf compatibility (TensorFlow 2.19.0 requires protobuf==3.20.3)...\")\n",
    "subprocess.run([\"pip\", \"install\", \"--force-reinstall\", \"protobuf==3.20.3\"], check=True)\n",
    "print(\"‚úÖ Protobuf fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix protobuf compatibility issue (if needed)\n",
    "# If you see AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
    "# Uncomment the following line:\n",
    "# !pip install protobuf==3.20.3\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Check protobuf version\n",
    "try:\n",
    "    import google.protobuf\n",
    "    print(\"Protobuf version:\", google.protobuf.__version__)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ (Optional) Download or prepare dataset\n",
    "\n",
    "Edit this cell if your training script expects data in a specific path.\n",
    "For example, you can mount Google Drive or download from Kaggle here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: mount Google Drive if your data is stored there.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Example: create a data directory\n",
    "# os.makedirs('data', exist_ok=True)\n",
    "# Then copy or download your dataset into ./data\n",
    "\n",
    "print(\"Dataset preparation step: customize as needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Run training\n",
    "\n",
    "This cell tries to run `train.py` at repo root.\n",
    "If your main script is at a different path, edit accordingly (e.g. `src/train.py`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe73708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/content/drive/MyDrive/TinyML_models\")  # CSVÎì§Ïù¥ ÏûàÎäî Í≤ΩÎ°úÎ°ú ÏàòÏ†ï\n",
    "csv_paths = sorted(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "dfs = [pd.read_csv(p, low_memory=False) for p in csv_paths]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Total Samples:\", len(df))\n",
    "print(df[\"attack\"].value_counts())\n",
    "print(df[\"attack\"].value_counts(normalize=True))  # ÎπÑÏú® ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Generate unique model filename with timestamp to avoid overwriting\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = f\"global_model_{timestamp}.h5\"\n",
    "model_path = f\"src/models/{model_filename}\"\n",
    "\n",
    "# Default entry point: Flower federated simulation with Colab config\n",
    "# Edit the command below if you want to run a different training script.\n",
    "print(f\"Running python -m src.federated.client --config config/federated_colab.yaml --save-model {model_path}\")\n",
    "!python -m src.federated.client --config config/federated_colab.yaml --save-model {model_path}\n",
    "\n",
    "# Also save as latest for easy access\n",
    "latest_path = \"src/models/global_model.h5\"\n",
    "if os.path.exists(model_path):\n",
    "    import shutil\n",
    "    shutil.copy(model_path, latest_path)\n",
    "    print(f\"\\n‚úÖ Also saved as latest: {latest_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694b385",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£.5 FGSM Adversarial Attack Testing\n",
    "\n",
    "Test FGSM (Fast Gradient Sign Method) attack on the trained model.\n",
    "This evaluates the model's robustness against adversarial examples.\n",
    "\n",
    "**Note:** This requires a trained model from the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FGSM attack testing\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Add project directory to Python path\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "# Check if model exists\n",
    "model_path = \"src/models/global_model.h5\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"‚ö†Ô∏è  WARNING: No trained model found!\")\n",
    "    print(\"‚ö†Ô∏è  Please run the training step (Cell 17) first.\")\n",
    "    print(\"‚ö†Ô∏è  The script will train a quick test model, but results will be less accurate.\")\n",
    "    print()\n",
    "\n",
    "# Run FGSM attack test\n",
    "print(\"Running FGSM attack testing...\")\n",
    "print(\"This may take a few minutes depending on dataset size.\\n\")\n",
    "\n",
    "!python scripts/test_fgsm_attack.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Save trained model(s) to Google Drive\n",
    "\n",
    "This will look for common output filenames (e.g. `tiny_model.tflite`) in the project root and copy them to your Drive.\n",
    "Edit `OUTPUT_FILES` if your script uses different names or locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516561b",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Compression Analysis\n",
    "\n",
    "Analyze model size, accuracy, and inference speed at each compression stage.\n",
    "Generate visualizations and reports.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10834c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export trained model to TFLite (optional, for comparison)\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "with open(\"config/federated_colab.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Load trained model\n",
    "model_path = \"src/models/global_model.h5\"\n",
    "if os.path.exists(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Export to TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save\n",
    "    tflite_path = \"src/models/global_model.tflite\"\n",
    "    with open(tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"‚úÖ Saved TFLite model: {tflite_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Model not found: {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfa898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run compression analysis\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Add project directory to Python path\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "# Analyze models\n",
    "models_to_analyze = [\"Baseline:src/models/global_model.h5\"]\n",
    "\n",
    "# Add TFLite if it exists\n",
    "if os.path.exists(\"src/models/global_model.tflite\"):\n",
    "    models_to_analyze.append(\"TFLite:src/models/global_model.tflite\")\n",
    "\n",
    "models_str = \" \".join([f'\"{m}\"' for m in models_to_analyze])\n",
    "\n",
    "cmd = f\"\"\"python scripts/analyze_compression.py \\\n",
    "    --models {models_str} \\\n",
    "    --baseline src/models/global_model.h5 \\\n",
    "    --config config/federated_colab.yaml \\\n",
    "    --output-dir data/processed/analysis \\\n",
    "    --format all\"\"\"\n",
    "\n",
    "print(\"Running compression analysis...\")\n",
    "print(f\"Command: {cmd}\\n\")\n",
    "print(f\"Python path: {sys.path[:3]}...\\n\")\n",
    "!{cmd}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Add project directory to Python path (if not already added)\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "results_path = \"data/processed/analysis/compression_analysis.csv\"\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    print(\"Generating visualizations...\")\n",
    "    !python scripts/visualize_results.py \\\n",
    "        --results {results_path} \\\n",
    "        --output-dir data/processed/analysis \\\n",
    "        --plot all\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Results file not found: {results_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47488b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display visualizations inline\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "analysis_dir = \"data/processed/analysis\"\n",
    "plots = [\n",
    "    \"size_vs_accuracy.png\",\n",
    "    \"compression_metrics.png\",\n",
    "    \"compression_ratio.png\"\n",
    "]\n",
    "\n",
    "for plot in plots:\n",
    "    plot_path = os.path.join(analysis_dir, plot)\n",
    "    if os.path.exists(plot_path):\n",
    "        print(f\"\\n## {plot}\")\n",
    "        display(Image(plot_path))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Plot not found: {plot_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776cca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy analysis results to Google Drive\n",
    "import shutil\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "analysis_dir = \"data/processed/analysis\"\n",
    "drive_dir = \"/content/drive/MyDrive/TinyML_models/analysis\"\n",
    "\n",
    "# Create directory\n",
    "os.makedirs(drive_dir, exist_ok=True)\n",
    "\n",
    "# Copy all analysis files\n",
    "found_any = False\n",
    "if os.path.exists(analysis_dir):\n",
    "    for file in os.listdir(analysis_dir):\n",
    "        src = os.path.join(analysis_dir, file)\n",
    "        dst = os.path.join(drive_dir, file)\n",
    "        if os.path.isfile(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"‚úÖ Copied: {file}\")\n",
    "            found_any = True\n",
    "\n",
    "if found_any:\n",
    "    print(f\"\\n‚úÖ All analysis results saved to: {drive_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No analysis files found in {analysis_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = \"/content/TinyML\"\n",
    "OUTPUT_FILES = [\n",
    "    \"src/models/global_model.h5\",  # Latest model (always copied)\n",
    "    \"src/models/global_model.tflite\",\n",
    "    \"models/global_model.h5\",\n",
    "    \"models/global_model.tflite\",\n",
    "    \"tiny_model.tflite\",\n",
    "    \"model.tflite\",\n",
    "    \"model.h5\",\n",
    "    \"saved_model.pb\"\n",
    "]\n",
    "\n",
    "# Also find all timestamped models to preserve history\n",
    "models_dir = Path(PROJECT_DIR) / \"src\" / \"models\"\n",
    "if models_dir.exists():\n",
    "    timestamped_models = list(models_dir.glob(\"global_model_*.h5\"))\n",
    "    if timestamped_models:\n",
    "        OUTPUT_FILES.extend([f\"src/models/{f.name}\" for f in timestamped_models])\n",
    "        print(f\"üì¶ Found {len(timestamped_models)} timestamped model(s) to preserve\")\n",
    "\n",
    "# Mount Drive to store the trained models\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "dest_dir = \"/content/drive/MyDrive/TinyML_models\"\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "# Create src/models subdirectory in Drive to preserve structure\n",
    "drive_src_dir = os.path.join(dest_dir, \"src\", \"models\")\n",
    "os.makedirs(drive_src_dir, exist_ok=True)\n",
    "\n",
    "found_any = False\n",
    "for fname in OUTPUT_FILES:\n",
    "    src_path = os.path.join(PROJECT_DIR, fname)\n",
    "    if os.path.exists(src_path):\n",
    "        # Keep directory structure in Drive\n",
    "        dst_path = os.path.join(dest_dir, fname)\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        print(f\"‚úÖ Copied {fname}\")\n",
    "        found_any = True\n",
    "\n",
    "if not found_any:\n",
    "    print(\"‚ö†Ô∏è No known model files found. Make sure your training script saves a model and update OUTPUT_FILES if needed.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All models saved to: {dest_dir}/src/models/\")\n",
    "    print(\"   üìå Timestamped models are preserved (no overwriting)\")\n",
    "    print(\"   üìå Latest model: global_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
